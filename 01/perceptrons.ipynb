{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Calculate the step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Prediction:\n",
    "$$\n",
    "ŷ = f(\\sum(w*x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Squared error:\n",
    "$$\n",
    "E  = \\sum(y-ŷ)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Function to minimize. The free variable is w.\n",
    "$$\n",
    "E \\equiv \\frac{1}{2}\\sum(y-f(\\sum(w*x)))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Applying chain rule:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial w} &= \\frac{\\partial \\frac{\\sum(y-ŷ)^2}{2}}{\\partial ŷ} * \\frac{\\partial -f(\\sum(w*x))}{\\partial \\sum(w*x)} * \\frac{\\partial \\sum(w*x)}{\\partial w} \\\\\n",
    "\\\\\n",
    " &= -(y-ŷ)*f'(\\sum(w*x)*x \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Sigmoid derivative\n",
    "$$\n",
    "f(x)=1/(1+e^{−x}) \\rightarrow f'(h)=f(h)(1−f(h))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Updating weights:\n",
    "$$\n",
    "\\Delta w = -\\eta \\frac{\\partial E}{w}\n",
    "$$\n",
    "\n",
    "*Note: the derivative measures instant increment. Since we want to reduce the error, we change the direction, hence the '$-$' symbol.*\n",
    "\n",
    "Weights increment:\n",
    "$$\n",
    "\\Delta w = \\eta*(y-ŷ)*ŷ*(1-ŷ)*x \\\\\n",
    "\\Delta w = \\eta \\delta x\n",
    "$$\n",
    "* $\\eta$: learning rate\n",
    "* $\\delta$: $(y-ŷ)*ŷ*(1-ŷ)$ it is error dependant\n",
    "* $x$: sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Calculate one gradient descent step for each weight\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = sigmoid(w.dot(x.reshape(2,1)))\n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = y-nn_output\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = learnrate * error * nn_output * (1 - nn_output) * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Gradient Loop implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data prep\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('data/binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train loss: ', 0.19696267698081696)\n",
      "('Train loss: ', 0.19696238351582482)\n",
      "('Train loss: ', 0.19696218155383541)\n",
      "('Train loss: ', 0.19696204235674888)\n",
      "('Train loss: ', 0.19696194630046757)\n",
      "('Train loss: ', 0.19696187994685732)\n",
      "('Train loss: ', 0.19696183407241416)\n",
      "('Train loss: ', 0.19696180233423277)\n",
      "('Train loss: ', 0.19696178036339629)\n",
      "('Train loss: ', 0.19696176514665412)\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    # TODO: Calculate the output\n",
    "    output = sigmoid(weights.dot(features.as_matrix().T))\n",
    "    # TODO: Calculate the error\n",
    "    error = targets-output\n",
    "    # TODO: Update weights\n",
    "    weights += (error * output * (1 - output)).dot(features.as_matrix())/n_records*learnrate\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Multilayer Perceptrons\n",
    "\n",
    "Below, you'll implement a forward pass through a 4x3x2 network, with sigmoid activation functions for both layers.\n",
    "\n",
    "Things to do:\n",
    "\n",
    "* Calculate the input to the hidden layer.\n",
    "* Calculate the hidden layer output.\n",
    "* Calculate the input to the output layer.\n",
    "* Calculate the output of the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_in_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_out = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = X\n",
    "hidden_layer_out = sigmoid(weights_in_hidden.T.dot(hidden_layer_in))\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = hidden_layer_out\n",
    "output_layer_out = sigmoid(weights_hidden_out.T.dot(output_layer_in))\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Error at the output layer in the neuron $k$\n",
    "$$\\delta^0_k$$\n",
    "\n",
    "Error at the hidden layer $h$ in the neuron $j$\n",
    "$$\\delta^h_j = \\sum W_{jk} \\delta^0_k f'(h_j)$$ \n",
    "\n",
    "Gradient descent step\n",
    "$$\\Delta_{ij} = \\eta \\delta_{output} V_{in}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "* Real value: $y=1$\n",
    "* Learning rate: $\\eta = 0.5$\n",
    "* Output of the hidden unit: $a$\n",
    "\n",
    "![Network](data/backprop-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction (forwards)**\n",
    "\n",
    "Hidden unit \n",
    "$$a = sigmoid(0.4*0.1 + (-0.2)*0.3) = 0.495$$\n",
    "\n",
    "Output unit\n",
    "$$ŷ = sigmoid( 0.1* a) = 0.512$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating gradient errors (backwards)**\n",
    "\n",
    "Output unit\n",
    "$$\\delta^0_0 = (y-ŷ)*f'(W · a)=(1-0.512)*0.512*(1-0.512)=0.122$$\n",
    "\n",
    "Hidden unit\n",
    "$$\\delta^1_0 = W \\delta^1_0f'(h) = 0.1 * 0.122 *0.495 *(1-0.495)=0.003$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Updating weights (backwards)**\n",
    "\n",
    "Output unit\n",
    "$$\\Delta^0_0 = \\eta \\delta^0_0a = 0.5*0.122*0.495 = 0.0302$$\n",
    "\n",
    "Hidden unit\n",
    "$$\\Delta^1_0 = \\eta \\delta^1_0 X = 0.5*0.003* [0.1, 0.3] = [0.00015, 0.00045]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Below, you'll implement the code to calculate one backpropagation update step for two sets of weights. I wrote the forward pass, your goal is to code the backward pass.\n",
    "\n",
    "Things to do:\n",
    "* Calculate the network error.\n",
    "* Calculate the output layer error gradient.\n",
    "* Use backpropagation to calculate the hidden layer error.\n",
    "* Calculate the weight update steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in weights for hidden layer to output layer:\n",
      "[ 0.00804047  0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[  1.77005547e-04  -5.11178506e-04]\n",
      " [  3.54011093e-05  -1.02235701e-04]\n",
      " [ -7.08022187e-05   2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "## Backwards pass\n",
    "## TODO: Calculate error\n",
    "error = target - output\n",
    "\n",
    "# TODO: Calculate error gradient for output layer\n",
    "del_err_output = error * output * (1-output)\n",
    "\n",
    "# TODO: Calculate error gradient for hidden layer\n",
    "del_err_hidden = weights_hidden_output * del_err_output * hidden_layer_output * (1-hidden_layer_output)\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * del_err_output * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_o = np.outer(learnrate * del_err_hidden,x).T\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Now you're going to implement the backprop algorithm for a network trained on the graduate school admission data. You should have everything you need from the previous exercises to complete this one.\n",
    "\n",
    "Your goals here:\n",
    "* Implement the forward pass.\n",
    "* Implement the backpropagation algorithm.\n",
    "* Update the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm (vectorial)**\n",
    "\n",
    "Calculate the prediction \n",
    "$$Ŷ$$\n",
    "\n",
    "Calculate the error gradient\n",
    "$$\\delta^0 = (Y-Ŷ)*f'(W*A)$$\n",
    "Where $A$ is the input to the output unit.\n",
    "\n",
    "Propagate the errors to the hidden layer and calculate gradient\n",
    "$$\\delta^h_j = \\delta^0 * W_j * f'(h_j)$$\n",
    "\n",
    "Update weights\n",
    "$$ W_j = W_j + \\eta \\frac{\\delta^h_j*a_i}{m} $$\n",
    "\n",
    "Where m is the number of samples.\n",
    "\n",
    "Repeat for $e$ epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('data/binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 3  # number of hidden units\n",
    "epochs = 500\n",
    "learnrate = 0.05\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Train loss: ', 0.24086081610559487)\n",
      "('Train loss: ', 0.23855446529706201)\n",
      "('Train loss: ', 0.23660964549959923)\n",
      "('Train loss: ', 0.23495534274939658)\n",
      "('Train loss: ', 0.23353501302739446)\n",
      "('Train loss: ', 0.23230369096005926)\n",
      "('Train loss: ', 0.23122561412251344)\n",
      "('Train loss: ', 0.23027230822025849)\n",
      "('Train loss: ', 0.22942106207352045)\n",
      "('Train loss: ', 0.22865372168644621)\n",
      "Prediction accuracy: 0.750\n"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    ## Forward pass ##\n",
    "    # TODO: Calculate the output\n",
    "    hidden_input = features.as_matrix().dot(weights_input_hidden)\n",
    "    hidden_activations = sigmoid(hidden_input)\n",
    "    output = sigmoid(hidden_activations.dot(weights_hidden_output))\n",
    "    \n",
    "    # Backward pass ##\n",
    "    # TODO: Calculate the error\n",
    "    error = targets.as_matrix()-output\n",
    "\n",
    "    # TODO: Calculate error gradient in output unit\n",
    "    output_error = error*output*(1-output)\n",
    "    \n",
    "    # TODO: propagate errors to hidden layer\n",
    "    hidden_error = output_error.reshape(360,1).dot(weights_hidden_output.reshape(1,3))*hidden_activations*(1-hidden_activations)\n",
    "\n",
    "    # TODO: Update weights\n",
    "    weights_input_hidden += learnrate*features.as_matrix().T.dot(hidden_error)/n_records\n",
    "    weights_hidden_output += learnrate*hidden_activations.T.dot(output_error)/n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_activations = sigmoid(np.dot(features, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_activations,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reads\n",
    "\n",
    "* [Karpathy - Yes you should understand backprop (Text)](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)\n",
    "* [Karpathy - CS231n Winter 2016: Lecture 5: Neural Networks Part 2  (Video)](https://www.youtube.com/watch?v=gYpoJMlgyXA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
